/**
 * Adaptador para Ollama API
 * 
 * Ollama fornece uma API REST local para interagir com modelos LLM.
 * API base: http://localhost:11434/api
 */

import { Socket } from 'socket.io';
import { AgentConfig } from '../../agents/config';
import { LLMAdapter, LLMThread, LLMMessage, LLMRun, TokenUsage } from './LLMAdapter';
import { executeTool } from '../../agents/agentManager';
import { emitToMonitors } from '../../services/monitoringService';
import { formatActionMessage } from '../../utils/functionDescriptions';

export interface OllamaConfig {
  baseUrl?: string; // URL base do Ollama (padr√£o: http://localhost:11434)
  defaultModel?: string; // Modelo padr√£o (padr√£o: llama2)
}

interface CachedAgent {
  id: string;
  instructions: string;
  tools: any[];
  model: string;
}

interface OllamaMessage {
  role: 'user' | 'assistant' | 'system';
  content: string;
}

interface OllamaChatRequest {
  model: string;
  messages: OllamaMessage[];
  stream?: boolean;
  options?: {
    temperature?: number;
    top_p?: number;
    top_k?: number;
  };
}

interface OllamaChatResponse {
  message: {
    role: string;
    content: string;
  };
  done: boolean;
  total_duration?: number;
  prompt_eval_count?: number;
  eval_count?: number;
}

// Armazena threads em mem√≥ria (Ollama n√£o tem conceito de threads)
const threadStore: Map<string, { messages: LLMMessage[]; created_at: number }> = new Map();
const agentStore: Map<string, CachedAgent> = new Map();
const runStore: Map<string, LLMRun> = new Map();

export class OllamaAdapter implements LLMAdapter {
  readonly provider = 'ollama';
  private baseUrl: string;
  private defaultModel: string;

  constructor(config?: OllamaConfig) {
    this.baseUrl = config?.baseUrl || 'http://localhost:11434';
    this.defaultModel = config?.defaultModel || 'llama2';
  }

  isConfigured(): boolean {
    // Ollama n√£o precisa de credenciais, mas verifica se est√° acess√≠vel
    // Nota: Esta verifica√ß√£o √© s√≠ncrona, ent√£o retorna true
    // A valida√ß√£o real ser√° feita quando tentar usar o adapter
    return true;
  }

  /**
   * Verifica se o Ollama est√° rodando e acess√≠vel
   */
  private async checkOllamaRunning(): Promise<{ isRunning: boolean; error?: string }> {
    try {
      const controller = new AbortController();
      const timeoutId = setTimeout(() => controller.abort(), 5000); // Timeout de 5 segundos
      
      const response = await fetch(`${this.baseUrl}/api/tags`, {
        method: 'GET',
        signal: controller.signal,
      });
      
      clearTimeout(timeoutId);
      
      if (!response.ok) {
        return {
          isRunning: false,
          error: `Ollama retornou status ${response.status}. Verifique se o servidor est√° rodando corretamente.`
        };
      }
      
      return { isRunning: true };
    } catch (error: any) {
      let errorMessage = 'Erro desconhecido ao conectar ao Ollama';
      
      if (error.name === 'AbortError') {
        errorMessage = 'Timeout ao conectar ao Ollama. O servidor pode estar lento ou n√£o estar respondendo.';
      } else if (error.code === 'ECONNREFUSED' || error.message?.includes('ECONNREFUSED')) {
        errorMessage = `N√£o foi poss√≠vel conectar ao Ollama em ${this.baseUrl}. Certifique-se de que o Ollama est√° rodando.`;
      } else if (error.message) {
        errorMessage = error.message;
      }
      
      console.warn(`‚ö†Ô∏è Ollama n√£o est√° acess√≠vel em ${this.baseUrl}:`, error);
      return { isRunning: false, error: errorMessage };
    }
  }

  /**
   * Compara se duas configura√ß√µes de tools s√£o iguais
   */
  private toolsEqual(tools1: any[], tools2: any[]): boolean {
    if (tools1.length !== tools2.length) return false;
    const tools1Str = JSON.stringify(tools1.sort((a, b) => (a.type || '').localeCompare(b.type || '')));
    const tools2Str = JSON.stringify(tools2.sort((a, b) => (a.type || '').localeCompare(b.type || '')));
    return tools1Str === tools2Str;
  }

  async getOrCreateAgent(config: AgentConfig): Promise<string> {
    // Verifica cache
    if (agentStore.has(config.name)) {
      const cached = agentStore.get(config.name)!;
      
      const instructionsChanged = cached.instructions !== config.instructions;
      const toolsChanged = !this.toolsEqual(cached.tools || [], config.tools || []);
      const modelChanged = cached.model !== (config.model || this.defaultModel);
      
      if (instructionsChanged || toolsChanged || modelChanged) {
        // Atualiza cache
        agentStore.set(config.name, {
          id: cached.id,
          instructions: config.instructions,
          tools: config.tools || [],
          model: config.model || this.defaultModel,
        });
        console.log(`üîÑ Agente "${config.name}" atualizado no cache`);
      }
      
      return cached.id;
    }

    // Cria novo agente no cache
    const agentId = `ollama_agent_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    agentStore.set(config.name, {
      id: agentId,
      instructions: config.instructions,
      tools: config.tools || [],
      model: config.model || this.defaultModel,
    });

    console.log(`‚úÖ Agente "${config.name}" criado (ID: ${agentId})`);
    return agentId;
  }

  async createThread(metadata?: Record<string, any>): Promise<LLMThread> {
    const threadId = `ollama_thread_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;
    threadStore.set(threadId, {
      messages: [],
      created_at: Date.now(),
    });

    return {
      id: threadId,
      created_at: Date.now(),
      metadata,
    };
  }

  async retrieveThread(threadId: string): Promise<LLMThread> {
    const thread = threadStore.get(threadId);
    if (!thread) {
      throw new Error(`Thread ${threadId} n√£o encontrada`);
    }

    return {
      id: threadId,
      created_at: thread.created_at,
    };
  }

  async addMessage(
    threadId: string,
    role: 'user' | 'assistant' | 'system',
    content: string
  ): Promise<LLMMessage> {
    const thread = threadStore.get(threadId);
    if (!thread) {
      throw new Error(`Thread ${threadId} n√£o encontrada`);
    }

    const message: LLMMessage = {
      id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
      role,
      content,
      created_at: Date.now(),
    };

    thread.messages.push(message);
    return message;
  }

  async listMessages(threadId: string, limit?: number): Promise<LLMMessage[]> {
    const thread = threadStore.get(threadId);
    if (!thread) {
      return [];
    }

    const messages = thread.messages;
    return limit ? messages.slice(-limit) : messages;
  }

  async createRun(threadId: string, assistantId: string, socket?: Socket): Promise<LLMRun> {
    const thread = threadStore.get(threadId);
    if (!thread) {
      throw new Error(`Thread ${threadId} n√£o encontrada`);
    }

    const agent = Array.from(agentStore.values()).find(a => a.id === assistantId);
    if (!agent) {
      throw new Error(`Agente ${assistantId} n√£o encontrado`);
    }

    const runId = `run_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;

    const run: LLMRun = {
      id: runId,
      thread_id: threadId,
      assistant_id: assistantId,
      status: 'queued',
      created_at: Date.now(),
    };

    // Armazena o run
    runStore.set(runId, run);

    // Processa o run de forma ass√≠ncrona
    this.processRun(run, thread, agent, socket).catch(error => {
      console.error(`‚ùå Erro ao processar run ${runId}:`, error);
    });

    return run;
  }

  private async processRun(
    run: LLMRun,
    thread: { messages: LLMMessage[] },
    agent: CachedAgent,
    socket?: Socket
  ): Promise<void> {
    try {
      run.status = 'in_progress';
      run.started_at = Date.now();

      // Prepara mensagens para o Ollama
      const ollamaMessages: OllamaMessage[] = [];
      
      // Adiciona instru√ß√µes do sistema se houver
      if (agent.instructions) {
        ollamaMessages.push({
          role: 'system',
          content: agent.instructions,
        });
      }

      // Adiciona hist√≥rico de mensagens
      for (const msg of thread.messages) {
        if (msg.role === 'user' || msg.role === 'assistant') {
          ollamaMessages.push({
            role: msg.role,
            content: msg.content,
          });
        }
      }

      // Verifica se o Ollama est√° rodando antes de fazer a chamada
      const checkResult = await this.checkOllamaRunning();
      if (!checkResult.isRunning) {
        const errorMsg = checkResult.error || `Ollama n√£o est√° acess√≠vel em ${this.baseUrl}`;
        const fullMessage = `${errorMsg}\n\n` +
          `Para resolver este problema:\n` +
          `1. Certifique-se de que o Ollama est√° instalado e rodando\n` +
          `2. Verifique se o Ollama est√° acess√≠vel em ${this.baseUrl}\n` +
          `3. Tente executar: ollama serve (se estiver usando linha de comando)\n` +
          `4. Verifique se a porta 11434 n√£o est√° sendo usada por outro processo\n` +
          `5. Se estiver usando uma URL diferente, configure-a nas configura√ß√µes do sistema`;
        throw new Error(fullMessage);
      }

      // Determina qual modelo usar
      // Se o modelo do agente for um modelo da OpenAI, usa o modelo padr√£o do Ollama
      let modelToUse = agent.model;
      const openaiModels = ['gpt-4', 'gpt-4-turbo', 'gpt-4-turbo-preview', 'gpt-3.5-turbo', 'gpt-3.5'];
      if (openaiModels.some(openaiModel => agent.model.toLowerCase().includes(openaiModel.toLowerCase()))) {
        modelToUse = this.defaultModel;
        console.log(`‚ö†Ô∏è Modelo do agente "${agent.model}" n√£o √© compat√≠vel com Ollama. Usando modelo padr√£o: ${this.defaultModel}`);
      }

      // Chama API do Ollama
      const response = await fetch(`${this.baseUrl}/api/chat`, {
        method: 'POST',
        headers: {
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          model: modelToUse,
          messages: ollamaMessages,
          stream: false,
        } as OllamaChatRequest),
      });

      if (!response.ok) {
        const errorText = await response.text().catch(() => response.statusText);
        let errorMessage = `Ollama API error (${response.status}): ${errorText}`;
        
        // Melhora mensagem de erro para modelo n√£o encontrado
        if (response.status === 404 && errorText.includes('not found')) {
          errorMessage = `Modelo "${modelToUse}" n√£o encontrado no Ollama.\n\n` +
            `Para resolver:\n` +
            `1. Verifique se o modelo est√° instalado: ollama list\n` +
            `2. Baixe o modelo: ollama pull ${modelToUse}\n` +
            `3. Ou configure um modelo diferente nas configura√ß√µes do sistema\n` +
            `4. Modelos dispon√≠veis: llama2, llama3, mistral, codellama, etc.`;
        }
        
        throw new Error(errorMessage);
      }

      const data = await response.json() as OllamaChatResponse;
      
      // Adiciona resposta √† thread
      const assistantMessage: LLMMessage = {
        id: `msg_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`,
        role: 'assistant',
        content: data.message.content,
        created_at: Date.now(),
      };

      thread.messages.push(assistantMessage);
      
      // Armazena informa√ß√µes de tokens se dispon√≠veis (para uso posterior)
      if (data.prompt_eval_count !== undefined && data.eval_count !== undefined) {
        (assistantMessage as any).tokenInfo = {
          promptTokens: data.prompt_eval_count,
          completionTokens: data.eval_count,
          totalTokens: data.prompt_eval_count + data.eval_count,
        };
      }

      // Emite para socket se dispon√≠vel
      if (socket) {
        socket.emit('agent_response_chunk', {
          content: data.message.content,
          done: true,
        });
      }

      run.status = 'completed';
      run.completed_at = Date.now();
      
      // Atualiza o run no store
      runStore.set(run.id, run);
    } catch (error: any) {
      run.status = 'failed';
      run.failed_at = Date.now();
      run.last_error = {
        code: 'ollama_error',
        message: error.message || 'Erro ao processar run',
      };
      // Atualiza o run no store mesmo em caso de erro
      runStore.set(run.id, run);
      throw error;
    }
  }

  async retrieveRun(threadId: string, runId: string): Promise<LLMRun> {
    const run = runStore.get(runId);
    if (run) {
      return run;
    }
    // Se n√£o encontrado, retorna um run gen√©rico
    return {
      id: runId,
      thread_id: threadId,
      assistant_id: '',
      status: 'completed',
    };
  }

  async waitForRunCompletion(
    threadId: string,
    runId: string,
    socket?: Socket
  ): Promise<{ message: string; tokenUsage: TokenUsage }> {
    const thread = threadStore.get(threadId);
    if (!thread) {
      throw new Error(`Thread ${threadId} n√£o encontrada`);
    }

    // Aguarda o run ser processado (polling)
    const maxWaitTime = 60000; // 60 segundos m√°ximo
    const startTime = Date.now();
    const pollInterval = 200; // Verifica a cada 200ms

    while (Date.now() - startTime < maxWaitTime) {
      const run = runStore.get(runId);
      
      if (run && (run.status === 'completed' || run.status === 'failed')) {
        break;
      }
      
      await new Promise(resolve => setTimeout(resolve, pollInterval));
    }

    // Busca a √∫ltima mensagem do assistente
    const messages = thread.messages.filter(m => m.role === 'assistant');
    const lastMessage = messages[messages.length - 1];

    if (!lastMessage) {
      const run = runStore.get(runId);
      if (run?.status === 'failed' && run.last_error) {
        throw new Error(run.last_error.message);
      }
      throw new Error('Nenhuma resposta do assistente encontrada');
    }

    // Usa informa√ß√µes de tokens se dispon√≠veis, sen√£o estima
    const tokenInfo = (lastMessage as any).tokenInfo;
    let tokenUsage: TokenUsage;
    
    if (tokenInfo) {
      tokenUsage = {
        promptTokens: tokenInfo.promptTokens,
        completionTokens: tokenInfo.completionTokens,
        totalTokens: tokenInfo.totalTokens,
      };
    } else {
      // Fallback: estima baseado no tamanho do texto
      const estimatedTokens = Math.ceil(lastMessage.content.length / 4);
      tokenUsage = {
        promptTokens: estimatedTokens,
        completionTokens: estimatedTokens,
        totalTokens: estimatedTokens * 2,
      };
    }
    
    return {
      message: lastMessage.content,
      tokenUsage,
    };
  }

  async submitToolOutputs(
    threadId: string,
    runId: string,
    toolOutputs: Array<{ tool_call_id: string; output: string }>
  ): Promise<LLMRun> {
    // Ollama n√£o suporta tool calling nativamente, mas podemos simular
    const thread = threadStore.get(threadId);
    if (!thread) {
      throw new Error(`Thread ${threadId} n√£o encontrada`);
    }

    // Adiciona outputs como mensagens do sistema
    for (const output of toolOutputs) {
      await this.addMessage(threadId, 'system', `Tool output: ${output.output}`);
    }

    return {
      id: runId,
      thread_id: threadId,
      assistant_id: '',
      status: 'completed',
    };
  }

  async listRuns(threadId: string, limit?: number): Promise<LLMRun[]> {
    // Ollama n√£o mant√©m hist√≥rico de runs
    return [];
  }

  async cancelRun(threadId: string, runId: string): Promise<LLMRun> {
    return {
      id: runId,
      thread_id: threadId,
      assistant_id: '',
      status: 'cancelled',
    };
  }
}

